<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Welcom to whmz&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="知识整理和分享">
<meta property="og:type" content="website">
<meta property="og:title" content="Welcom to whmz&#39;s Blog">
<meta property="og:url" content="http://whmz.github.io/index.html">
<meta property="og:site_name" content="Welcom to whmz&#39;s Blog">
<meta property="og:description" content="知识整理和分享">
<meta property="og:locale" content="zh-cn">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Welcom to whmz&#39;s Blog">
<meta name="twitter:description" content="知识整理和分享">
  
    <link rel="alternate" href="/atom.xml" title="Welcom to whmz&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Welcom to whmz&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">总结，进步，分享</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://whmz.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-ipmarks" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/10/ipmarks/" class="article-date">
  <time datetime="2018-03-10T13:05:06.000Z" itemprop="datePublished">2018-03-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/10/ipmarks/">一个流量标记问题导致的限速Bug</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>继续之前限速脚本的话题，本次整理下处理的一个限速Bug。</p>
<p>近期对网络出口做了扩容，按监控的带宽数据来算，目前有20%左右的裕量，按说用户那边不再那么卡了，但是这周总是接到用户投诉说网速达不到，并发了相应的截图。为了排查具体的情况，在周三、四做了两次的问题排查，终于发现了问题点。本文简单记录下问题发现和排查过程。</p>
<h2 id="问题时间点"><a href="#问题时间点" class="headerlink" title="问题时间点"></a>问题时间点</h2><p>我们运营的学校在这周开学，大概从周二开始，有用户投诉带宽问题。考虑带宽和限速模块没有做过调整，本来是很确信不会有问题，毕竟已经正常运行了那么久的系统了！但是周三，我们自己的客服也在反馈这个问题，并拿到了实际测试的结果：在各种带宽的测试环境下，基本达不到预期的值，特别是在50M环境下，一般还达不到20M。这样基本说明，问题点确实存在，需要详细排查下。</p>
<h2 id="第一次检查"><a href="#第一次检查" class="headerlink" title="第一次检查"></a>第一次检查</h2><p>周三因为要解决另外的问题，在观察问题流量的同时，对系统做了次简单的检查。大致的流程如下：</p>
<ol>
<li><p>检查用户拨号后，下发的限速值是否正确<br> 这个可以通过syslog日志输出的限速参数检查，发现测速有问题的账号，实际限速参数并没有问题，上行20M，下行50M。在检查后再次测速，速率仍有较大差异。</p>
</li>
<li><p>检查限速参数是否正确应用到了tc的正确队列<br> 按上篇文章的介绍，用户IP和tc队列有个确定的对应关系，tc_class = (srcIP &amp; 0xff) + 1, 根据这个计算出用户队列，检查对应的上下行参数，正确。<br> ‘’’<br>  |<br>  +—(10:1daf) htb prio 1 rate 20Mbit ceil 20Mbit burst 1600b cburst 1600b<br>  |             Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0) </p>
<h2 id="rate-0bit-0pps-backlog-0b-0p-requeues-0"><a href="#rate-0bit-0pps-backlog-0b-0p-requeues-0" class="headerlink" title="  |             rate 0bit 0pps backlog 0b 0p requeues 0 "></a>  |             rate 0bit 0pps backlog 0b 0p requeues 0 </h2><p>  |             rate 0bit 0pps backlog 0b 0p requeues 0<br>  |<br>  +—(10:1daf) htb prio 1 rate 50Mbit ceil 50Mbit burst 1600b cburst 1600b<br>  |             Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)<br>  |             rate 0bit 0pps backlog 0b 0p requeues 0<br> ‘’’</p>
</li>
<li><p>经过这两步检查，发现用户速率没有问题，这时候怀疑是不是上行带宽实际上没有扩容？<br> 使用了<a href="https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py" target="_blank" rel="noopener">speettest的脚本</a> 检查服务器带宽：<br> ‘’’<br> wget <a href="https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py" target="_blank" rel="noopener">https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py</a><br> python speedtest.py<br> ‘’’<br> 检查发现，在当时负载比较高、接近扩容前带宽的情况下，速率仍能达到1957Mbps，可以判断出口带宽已经扩容了。</p>
</li>
<li><p>这样，排除外部原因，开始检查下校园网络的情况。因为接入方式流量跑在校园网上，如果校园网情况不好，带宽也起不来。<br> 安排了客服下载校园内服务器的资源，可以跑满100M….</p>
</li>
</ol>
<p>经过这几步排查，发现问题确实存在，但排除了下发错误、限速参数设置、出口带宽、内部网络故障这几个原因，有可能是服务程序问题。这时候也有客服反馈，现场有单个账号出问题的，只有某个账号速率不足，50M只能到10Mbps，同样环境换账号就可以。这样差不多可以怀疑限速功能是有问题了。</p>
<p>之后是校园停电，12点了，没有办法继续试验排查，再加上有其他问题检查解决，也就暂时放下了这个问题。</p>
<h2 id="第二次检查"><a href="#第二次检查" class="headerlink" title="第二次检查"></a>第二次检查</h2><p>周四，在解决了其他问题后，重新开始检查限速的问题。这时候，因为排除了一些因素，重点放到了限速的功能上，但是还没相信是代码问题….大致做了这么些检查：</p>
<ol>
<li><p>确定一个账号先检查<br> 在这个用户登录后，获取其登录IP地址，10.151.25.99, 换算其tc-class为 10:1965, 检查其tc限速参数<br> ‘’’<br> []@vser:~$ sudo /etc/xlipsec/mark_tc.sh status ens3f0 ens3f1| grep -C 2 10:1965<br>  |             rate 0bit 0pps backlog 0b 0p requeues 0<br>  |<br>  +—(10:1965) htb prio 1 rate 20Mbit ceil 20Mbit burst 1600b cburst 1600b<br>  |             Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0) </p>
<h2 id="rate-0bit-0pps-backlog-0b-0p-requeues-0-1"><a href="#rate-0bit-0pps-backlog-0b-0p-requeues-0-1" class="headerlink" title="  |             rate 0bit 0pps backlog 0b 0p requeues 0 "></a>  |             rate 0bit 0pps backlog 0b 0p requeues 0 </h2><p>  |             rate 0bit 0pps backlog 0b 0p requeues 0<br>  |<br>  +—(10:1965) htb prio 1 rate 50Mbit ceil 50Mbit burst 1600b cburst 1600b<br>  |             Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)<br>  |             rate 0bit 0pps backlog 0b 0p requeues 0<br> ‘’’<br> 可以看到，限速的队列带宽没有问题，上行20M，下行50M</p>
</li>
<li><p>测速结果，上行8.05M、下行8.91M，完全不在同一水平上么<br> 这时候，我发现” Send 0 bytes 0 pkt “，而且上下行数据都是0，这说明流量根本就没走到这个队列！</p>
</li>
<li><p>考虑到限速映射过程只有两个步骤，标记、映射，先检查标记部分：<br> ‘’’<br> []@vser:~$ sudo iptables -t mangle -L -v<br> Chain POSTROUTING (policy ACCEPT 56G packets, 57T bytes)<br> pkts bytes target     prot opt in     out     source               destination<br> 18G 2164G IPMARK     all  –  any    any     10.151.0.0/18        anywhere             -j IPMARK –addr src  –and-mask 0xfff<br> 16G   27T IPMARK     all  –  any    any     anywhere             10.151.0.0/18        -j IPMARK –addr dst  –and-mask 0xfff<br> ‘’’</p>
<p> 看到这个结果就知道是哪里出问题了。地址池使用了18位掩码，就意味着变化的部分是后14位，如果使用0xfff掩码计算用户的mark值，就会导致部分高地址用户被映射到低位区间，跟其他用户冲突。按测试账号的数据，10.151.25.99，刚好是超出范围的高位IP用户，实际会被标记为 0965，与10.151.9.99用户共用了一个限速队列。</p>
<p> 当9.99这个用户在线的时候，两人实际上共用同一个队列，共享带宽，这就是有时候测速接近20M的原因，9.99用户使用的是20M套餐；而当9.99用户下线后，这个限速队列被重置为10Mbps，这时候测速就会出现先前的数据：上行8.05M、下行8.91M 。</p>
</li>
<li><p>问题找到后，对IPMARK代码做了修改，<br> ‘’’<br> iptables -t mangle -A POSTROUTING -s 10.151.0.0/18 -j IPMARK –addr src –and-mask 0x3fff<br> iptables -t mangle -D POSTROUTING 1<br> ‘’’</p>
<p> 这样再测试先前的账号，已经可以看到对应的 10:1965 队列已经有流量经过：<br> ‘’’</p>
<pre><code>|     
</code></pre><p>  +—(10:1965) htb prio 1 rate 10Mbit ceil 10Mbit burst 1600b cburst 1600b<br>  |             Sent 162249444 bytes 1574854 pkt (dropped 0, overlimits 0 requeues 0) </p>
<h2 id="rate-0bit-0pps-backlog-0b-0p-requeues-0-2"><a href="#rate-0bit-0pps-backlog-0b-0p-requeues-0-2" class="headerlink" title="  |             rate 0bit 0pps backlog 0b 0p requeues 0 "></a>  |             rate 0bit 0pps backlog 0b 0p requeues 0 </h2><p>  |             rate 0bit 0pps backlog 0b 0p requeues 0<br>  |<br>  +—(10:1965) htb prio 1 rate 20Mbit ceil 20Mbit burst 1600b cburst 1600b<br>  |             Sent 1362668930 bytes 1770932 pkt (dropped 0, overlimits 0 requeues 0)<br>  |             rate 0bit 0pps backlog 0b 0p requeues 0<br> ‘’’</p>
</li>
<li><p>修改后的测试结果是，speettest站点测速，下行只有27.58Mbps，上行为 11.30Mbps，仍然离实际限速值差距较大。<br>为了排查问题，对这个账号的限速参数做了调整，先后尝试了80M、8M、20M<br> ‘’’<br> sudo tc class replace dev ens3f0 parent 10:1 classid 10:1995 htb rate 80Mbit ceil 80Mbit prio 1<br> sudo tc class replace dev ens3f0 parent 10:1 classid 10:1995 htb rate 8Mbit ceil 8Mbit prio 1<br> sudo tc class replace dev ens3f0 parent 10:1 classid 10:1995 htb rate 20Mbit ceil 20Mbit prio 1<br> ‘’’<br> 测试结果显示，限速值较小时，测速结果接近限速数据，比如8M测速为6.99M，20M被限速为 16.97M； 当速率较大的时候，速率差距就很大了，比如80M测速为 24M。再看tc队列状态，每次测速，对应的tc队列数据都会增加，说明数据确实在这个队列被限速。但是对应的dropped数据始终为0，说明数据不是在这个被丢掉的。再加上当时系统负载比较低、测速时数据包延迟在7、8ms，可以推断不是在网关限速的。</p>
</li>
<li><p>联系客服换个测速站试试，试验了360，测试50M限速情况下，实际测速 5.25 * 8 = 42Mbps，考虑封包损耗，可以认为接近限速值了。另外就是，tc的dropped字段也终于出现了非0值。</p>
</li>
</ol>
<p>后续客服联系了speedtest的限速，请求排查限速的问题，怀疑是测速站点被限速了，但在等反馈结果。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>mark参数问题：最早设计时，单网关考虑4096个设备，准备采用20位地址掩码，这样计算用户的mark值只需要0xfff就够了。但是后期发现，尽量给同一账号的用户分配相同地址，这样实际占用的地址数多于在线用户数，最终使用了18位地址掩码、16384个地址，相应的mark掩码就需要调整为0x3fff。但是，部署发现问题后，只在master基线修改了代码，并没有更新部署包。</li>
</ol>
<p>新服务部署时，一般不会去检查、修改某个特殊的参数；在小规模测试时，也不会触发这个问题（登录设备大于4096才会触发问题），所以直到用户规模足够，才发现了问题。</p>
<ol>
<li>测速反应的是客户端到测速服务器的速度，只要360能达到限速值，就说明网关没限速；但对用户来说速率实际没有达到标称值。</li>
</ol>
<h2 id="改进措施"><a href="#改进措施" class="headerlink" title="改进措施"></a>改进措施</h2><ol>
<li>修改参数后，应及时评估相关改动，对所有涉及的数据做检查，修正数据；</li>
<li>修复Bug后，应及时发布新版本修改问题，整理升级脚本；</li>
<li>服务部署的版本应该登记、管理：在发现新Bug后，应该评估影响的服务器，对所有相关服务尽快执行升级；</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://whmz.github.io/2018/03/10/ipmarks/" data-id="cjelgshdo000110wiubj2r25n" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-ss-hfsc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/25/ss-hfsc/" class="article-date">
  <time datetime="2018-02-25T10:33:06.000Z" itemprop="datePublished">2018-02-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/25/ss-hfsc/">多用户环境下的流量标记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>单位使用了个Linux网关在园区网络提供旁路接入功能。作为运营网络，肯定得考虑限速的问题，使用了iptables、IPMARK、tc-htb等功能实现了限速。</p>
<p>随着用户数逐渐增多，当出口网络比较拥堵的时候，大量用户反馈延迟较高、游戏用户基本无法使用。基于这样的情况，我们尝试是否能在当前的系统上，增加QoS支持：区分用户的应用流量、执行不同速率策略，缓解问题。</p>
<p>本文简单记录了两个试验方案，使用HTB和HFSC限速和整流，给出限速的关键脚本和思路，以及配套的MARK规则。本文先不讨论HTB、HFSC本身的使用方法和设计，主要讨论分类器的设计和实现。</p>
<h2 id="基于用户的限速"><a href="#基于用户的限速" class="headerlink" title="基于用户的限速"></a>基于用户的限速</h2><p>策略比较简单，每个接入用户使用自己的htb-class：</p>
<ul>
<li>在用户接入的时候，Radius模块能收到用户的接入IP、上下行带宽数据，根据这些数据设置htb-class的限速参数</li>
<li>当用户有网络流量经过网关时，根据IP把对应的流量映射到htb-class</li>
</ul>
<p>在具体代码上，大致有这么三段：</p>
<ol>
<li><p>通过iptables-mark标记流量</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 根据流方向，用户上下行有两条规则.对于用户输入流量，假定用户IP为 srcIP ，则 </span><br><span class="line"># mark = srcIP &amp; 0xff</span><br><span class="line"></span><br><span class="line">iptables -t mangle -A POSTROUTING -s $VIPS -j IPMARK --addr src --and-mask 0xfff</span><br></pre></td></tr></table></figure>
</li>
<li><p>tc初始化    </p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 定义用户流量映射规则</span><br><span class="line"># 以mark为key，映射用户流量到 10:(1+mark) 分类</span><br><span class="line"></span><br><span class="line">tc filter add dev $ODEV parent 10: handle 100 flow map key mark baseclass 10:1</span><br></pre></td></tr></table></figure>
</li>
<li><p>tc限速设置</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 用户IP为vip，计算其vflag，对10:$vflag分类设置速率值，其中</span><br><span class="line"># vflag = ($vip + 1) &amp; 0xfff</span><br><span class="line"></span><br><span class="line">tc class replace dev $ODEV parent 10:1 classid 10:$vflag htb rate $urate ceil $uceil prio 1</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="基于用户-QoS的限速"><a href="#基于用户-QoS的限速" class="headerlink" title="基于用户+QoS的限速"></a>基于用户+QoS的限速</h2><p>根据我们的分析，除了扩大出口流量，试试对用户流量分级、降低大流量应用的优先级、保障游戏、交互带宽，应该也能缓解部分问题。</p>
<p>相对简单的用户限速方案，这个方案要在识别用户流量的基础上，进一步识别应用。考虑我们的流量规模大致在5~10Gbps，在这样的网络环境下要实现应用流量识别、再加以QoS限速，从性能上讲不大现实——算法太复杂，算法本身对性能的影响太大，会进一步加大延迟。</p>
<p>内部讨论后，我们基于包长度做了个简单的模型。除了数据包本身的QoS字段之外，包长度在一定程度上也能反映用户数据流量的特征：大数据传输一般使用最大报文长度；TCP初始连接、交互数据一般不会有太大的数据量，从而数据包长度较短。</p>
<p>考虑这个情况，我们的需求1：<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">设计一个相对简洁的标记机制，区分同一用户下的流量，使得针对不同的流量能施加不同的速率控制</span><br></pre></td></tr></table></figure></p>
<p>另外，用户接入过程也会占用一部分流量，这部分流量因为未纳入限速框架（用户接入成功前IP不在$VIP定义的地址池中），默认会归为系统流量。如果不给这部分流量保留带宽，用户在高峰期的连接成功率会比较差。需求2来了：<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">新的流量标记机制，必须区分系统带宽和用户带宽，对系统带宽也进行分类，对接入流量和其他流量设置不同策略。</span><br></pre></td></tr></table></figure></p>
<p>基于这两个需求，我们设计了这样的一个MARK规则（请忽略长度数据….）：<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># length &lt; 400</span><br><span class="line">1.  iptables -t mangle -A POSTROUTING -d $VIPS -m length --length :400 -j IPMARK --addr dst --and-mask 0x3fff --or-mask 0x24000</span><br><span class="line">2.  iptables -t mangle -A POSTROUTING -d $VIPS -m length --length :400 -j RETURN</span><br><span class="line"></span><br><span class="line"># length &gt; 400</span><br><span class="line">3.  iptables -t mangle -A POSTROUTING -d $VIPS -j IPMARK --addr dst --and-mask 0x3fff --or-mask 0x28000</span><br><span class="line">4.  iptables -t mangle -A POSTROUTING -d $VIPS -j RETURN</span><br><span class="line"></span><br><span class="line"># sys load</span><br><span class="line">5.  iptables -t mangle -A POSTROUTING -m length --length :400 -j IPMARK --set-mark 0x14000</span><br><span class="line">6.  iptables -t mangle -A POSTROUTING -m length --length :400 -j RETURN</span><br><span class="line"></span><br><span class="line">7.  iptables -t mangle -A POSTROUTING -j MARK --set-mark 0x18000</span><br><span class="line">8.  iptables -t mangle -A POSTROUTING -j RETURN</span><br></pre></td></tr></table></figure></p>
<p>按设计想法，用户流量符合$VIPS地址池条件，会在前面的四条规则中被设置为 ((ip &amp; 0x3fff) | 0x24000) 或者 ((ip &amp; 0x3fff) | 0x24000)，系统流量则归为 0x14000 和 0x18000 ，从而实现上述两个需求。同时，符合一条规则的包不再执行后续的检查，从性能上讲也好一点。</p>
<p>但是，实际测试时候发现，所有包都被纳入了 0x14000 和 0x18000 这两个分类！<br>为了跟踪包标记情况，使用了iptables -t mangle -L -v，查看了每条规则的包匹配情况，发现每个经过1、2的包，都会再次经过5、6; 而3、4两条规则也是一样，再次经过7、8。这样的话，每个包被标记了两次，肯定是后一个标记起作用。</p>
<p>那么问题是，为什么包在规则2、4行的RETURN之后，还会继续经过后续的5、6、7、8规则呢？原来，我们的接入方式是IPSec，一个网络包进入网卡后，会根据数据方向，通过XFRM框架解包/封包，在协议栈内部实际跑两圈，两次经过mangle postrouting链。在用户下行数据方向，就会出现刚才的情况，实际上所有流量都会走到系统队列处理。这个可以参考<a href="https://upload.wikimedia.org/wikipedia/commons/3/37/Netfilter-packet-flow.svg" target="_blank" rel="noopener">Netfilter-packet-flow</a></p>
<p>为了解决这个问题，修改后的的MARK规则是这样的:<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># ip addr</span><br><span class="line">iptables -t mangle -A POSTROUTING -d $VIPS -j IPMARK --addr dst --and-mask 0x3fff --or-mask 0x20000</span><br><span class="line"></span><br><span class="line"># load length</span><br><span class="line">iptables -t mangle -A POSTROUTING -m length --length :400 -j MARK --or-mark 0x14000</span><br><span class="line">iptables -t mangle -A POSTROUTING -m length --length :400 -j RETURN</span><br><span class="line"></span><br><span class="line">iptables -t mangle -A POSTROUTING -j MARK --or-mark 0x18000</span><br><span class="line">iptables -t mangle -A POSTROUTING -j RETURN</span><br></pre></td></tr></table></figure></p>
<p>按这个规则，地址池$VIPS中的地址包，会在原有的mark基础上添加一个0x20000; 所有的包都会根据包长度，在原有mark上添加一个 0x14000 或 0x18000 。这样，用户包，始终都会经过两次mark标记，最终结果为 0x34000 或 0x38000 ；系统包因为不会满足$VIPS的地址池条件，其mark只能是 0x14000 或 0x18000 。该标记满足了上述两个需求。</p>
<p>但是这样一来，每个用户包已经至少多执行3条过滤规则</p>
<h2 id="需要进一步学习的内容"><a href="#需要进一步学习的内容" class="headerlink" title="需要进一步学习的内容"></a>需要进一步学习的内容</h2><p>hfsc限速参数中，rt、ls、ul、sc的含义和相互关系。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://whmz.github.io/2018/02/25/ss-hfsc/" data-id="cjelgshdv000210wif714xv8o" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hello" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/25/hello/" class="article-date">
  <time datetime="2018-02-25T10:28:45.000Z" itemprop="datePublished">2018-02-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/25/hello/">开始！</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>用作个人知识整理和分享的Blog，目前规划这些内容：</p>
<ul>
<li>知识备忘：近段时间了解的新知识，稍作探索的摘要，或简单记录用作备忘。</li>
<li>学习整理：近期内学习、掌握的知识和技能点，做部分整理，以用作经验分享。</li>
<li>问题记录：记录平时工作学习解决的问题。一般来源于工作实践，记录处理具体问题的过程和方法。</li>
<li>学习计划：近段时间预计学习的知识点</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://whmz.github.io/2018/02/25/hello/" data-id="cjelgshc4000010wihwe3tml4" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">二月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/03/10/ipmarks/">一个流量标记问题导致的限速Bug</a>
          </li>
        
          <li>
            <a href="/2018/02/25/ss-hfsc/">多用户环境下的流量标记</a>
          </li>
        
          <li>
            <a href="/2018/02/25/hello/">开始！</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 whmz?<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>
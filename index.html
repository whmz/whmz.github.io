<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Welcom to whmz&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="知识整理和分享">
<meta property="og:type" content="website">
<meta property="og:title" content="Welcom to whmz&#39;s Blog">
<meta property="og:url" content="http://whmz.github.io/index.html">
<meta property="og:site_name" content="Welcom to whmz&#39;s Blog">
<meta property="og:description" content="知识整理和分享">
<meta property="og:locale" content="zh-cn">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Welcom to whmz&#39;s Blog">
<meta name="twitter:description" content="知识整理和分享">
  
    <link rel="alternate" href="/atom.xml" title="Welcom to whmz&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Welcom to whmz&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">总结，进步，分享</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://whmz.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Strongswan-DAE" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/11/Strongswan-DAE/" class="article-date">
  <time datetime="2018-03-11T13:17:06.000Z" itemprop="datePublished">2018-03-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/11/Strongswan-DAE/">Strongswan-DAE功能相关逻辑整理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li><p>概念</p>
<ul>
<li><p>DAE（Dynamic Authorization Extensions，动态授权扩展）协议是RFC 5176中定义的RADIUS协议的一个扩展，它用于强制认证用户下线，或者更改在线用户授权信息。DAE采用客户端/服务器通信模式，由DAE客户端和DAE服务器组成。</p>
<p>  DAE客户端：用于发起DAE请求，通常驻留在一个RADIUS服务器上，也可以为一个单独的实体。</p>
<p>  DAE服务器：用于接收并响应DAE客户端的DAE请求，通常为一个NAS（Network Access Server，网络接入服务器）设备。</p>
</li>
<li><p>DAE报文包括以下两种类型：</p>
<p>  DMs（Disconnect Messages）：用于强制用户下线。DAE客户端通过向NAS设备发送DM请求报文，请求NAS设备按照指定的匹配条件强制用户下线。</p>
<p>  COA（Change of Authorization）Messages：用于更改用户授权信息。DAE客户端通过向NAS设备发送COA请求报文，请求NAS设备按照指定的匹配条件更改用户授权信息。</p>
<p>  在设备上使能RADIUS DAE服务后，设备将作为RADIUS DAE服务器在指定的UDP端口监听指定的RADIUS DAE客户端发送的DAE请求消息，然后根据请求消息进行用户授权信息的修改或断开用户连接，并向RADIUS DAE客户端发送DAE应答消息。</p>
</li>
</ul>
</li>
<li><p>Strongswan-DAE 问题分析</p>
<ul>
<li><p>现象</p>
<ul>
<li>某个用户登录认证连续多次提示已在线，踢下线没有效果；</li>
<li>检查发现以下情况：<ul>
<li>日志显示：用户13：50：57 认证成功，13：50：59 下线成功</li>
<li>用户13：50之后一直access-reject，直到14：30左右成功登录</li>
<li>radius处检查后，认为是进入会话异常呆死状态，根据已有逻辑，大概30分钟后才能结束该会话</li>
<li>用户通过自理平台发起下线，14：09下线失败，返回 Dissconn-NAK；14：16有一次提示Dissconn-ACK，但是之后并没有实际踢下线；后续的踢下线都是NAK</li>
</ul>
</li>
</ul>
</li>
<li><p>检查步骤和发现的问题</p>
<ul>
<li><p>网关日志检查</p>
<ul>
<li>确认用户13：50：57 认证成功，13：50：59 下线成功；下线时发送了Accounting-END，并且收到了Radius的响应报文</li>
<li><p>确认14：16该用户有一次Dissconn-ACK报文，其他都是NAK</p>
<p>结论：需要检查下Strongswan的DAE处理逻辑，看是什么问题</p>
</li>
</ul>
</li>
<li><p>使用Source Insight查看strongswan代码</p>
<ul>
<li><p>搜索dae代码会出现大量的daemon相关的代码，结果太多</p>
</li>
<li><p>搜索dae、大小写敏感、只搜索注释部分，找到了DAE的处理文件：              \strongswan\src\libcharon\plugins\eap_radius\eap_radius_dae.c</p>
</li>
<li><p>找到了DAE-disconn处理入口：process_disconnect，关键代码：</p>
<ul>
<li>ids = get_matching_ike_sas(this, radius_message_t *request, client);<ul>
<li>根据request的属性信息查找对应的ike_sa</li>
</ul>
</li>
<li>后续代码：<ul>
<li>如果找到sa，则执行清理下线，发送给RMC_DISCONNECT_ACK报文；否则发送RMC_DISCONNECT_NAK。 打印日志信息的代码与日志匹配。确定是该处逻辑处理DAE</li>
</ul>
</li>
</ul>
</li>
<li><p>get_matching_ike_sas(this, radius_message_t *request, client):</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">while (enumerator-&gt;enumerate(enumerator, &amp;type, &amp;data))</span><br><span class="line">&#123;</span><br><span class="line">    if (type == RAT_USER_NAME &amp;&amp; data.len)</span><br><span class="line">    &#123;</span><br><span class="line">        user = identification_create_from_data(data);</span><br><span class="line">        DBG1(DBG_CFG, &quot;received RADIUS DAE %N for %Y from %H&quot;,</span><br><span class="line">            radius_message_code_names, request-&gt;get_code(request),</span><br><span class="line">            user, client);</span><br><span class="line">        add_matching_ike_sas(ids, user);</span><br><span class="line">        user-&gt;destroy(user);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>第一个关键点，处理dae-request时，仅使用了其中的RAT_USER_NAME数据，没有使用session-id… 结合用户行为，可以推测：</p>
<ul>
<li>用户边执行重试拨号、边执行了自理平台的下线操作</li>
</ul>
</li>
<li><p>检查日志信息，发现有以下信息：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    14:16:13 charon: 20[CFG] received RADIUS DAE Disconnect-Request for 1769136.... from 59.110.24.89</span><br><span class="line">    14:16:13 charon: 20[CFG] closing 1 IKE_SA matching Disconnect-Request, sending Disconnect-ACK</span><br><span class="line">    14:16:13 charon: 26[IKE] destroying IKE_SA in state CONNECTING without notification</span><br><span class="line">    ``` </span><br><span class="line">    网关认为是CONNECTING状态的会话，不需要做其他操作，因此不会给radius侧回复Account-END，这一点与Radius的日志信息匹配</span><br><span class="line"></span><br><span class="line">* 这个DAE使用用户名踢下线，具体会踢哪些会话，是否会全踢，还是只踢第一个：</span><br></pre></td></tr></table></figure>
<p>  ids = get_matching_ike_sas(this, request, client);</p>
<p>  if (ids-&gt;get_count(ids))<br>  {</p>
<pre><code>DBG1(DBG_CFG, &quot;closing %d IKE_SA%s matching %N, sending %N&quot;,
    ids-&gt;get_count(ids), ids-&gt;get_count(ids) &gt; 1 ? &quot;s&quot; : &quot;&quot;,
    radius_message_code_names, RMC_DISCONNECT_REQUEST,
    radius_message_code_names, RMC_DISCONNECT_ACK);

enumerator = ids-&gt;create_enumerator(ids);
while (enumerator-&gt;enumerate(enumerator, &amp;id))
{
    lib-&gt;processor-&gt;queue_job(lib-&gt;processor, (job_t*)
                            delete_ike_sa_job_create(id, TRUE));
}
enumerator-&gt;destroy(enumerator);

send_response(this, request, RMC_DISCONNECT_ACK, client);
</code></pre><p>  }<br>  else{</p>
<pre><code>...
</code></pre><p>  }</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">                    从代码看，get_matching_ike_sas返回的ids，都会执行清理操作。这一点需要注意，后续统一用户存在多个会话时，确实会存在误踢，把用户全部会话都踢掉</span><br><span class="line"></span><br><span class="line">    * DAE-COA</span><br><span class="line">        * 检查了COA的代码处理逻辑，process_coa,在处理时跟disconn一样，也只匹配了用户名字段，没有检查session-id是否一样。也就是说，后续如果启用COA功能，也面临一样的问题，会同时重置多个连接的在线时长。这一点可能还是有益的，当用户在线续费、或费用到期，可以同时停止或启用。</span><br><span class="line"></span><br><span class="line">3. DAE和超时时间</span><br><span class="line">    在实际执行中，发现有时候用户执行踢下线操作时，会延迟很长时间（几分钟），实际的下线操作才会成功。根据用户ID、IP、会话信息查找具体的会话DAE过程，发现这样一个情况：踢下线操作收到后，SS端先执行会话查找操作，找到会话后就发送ACK给Radius端，同时给会话的另一端发送一个断开连接的请求；如果用户还在线则很快响应这个报文下线；如果用户已经断开了网络（断开原先链接的Wifi、非断开IPSec连接），则会重发多次报文之后，才会执行断开操作。而只有断开操作成功后，才会给Radius端发送记账结束报文。</span><br><span class="line"></span><br><span class="line">    以下是超时时间的选择：</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="etc-strongswan-d-charon-conf"><a href="#etc-strongswan-d-charon-conf" class="headerlink" title="/etc/strongswan.d/charon.conf"></a>/etc/strongswan.d/charon.conf</h1><h1 id="Base-to-use-for-calculating-exponential-back-off-see-IKEv2-RETRANSMISSION"><a href="#Base-to-use-for-calculating-exponential-back-off-see-IKEv2-RETRANSMISSION" class="headerlink" title="Base to use for calculating exponential back off, see IKEv2 RETRANSMISSION"></a>Base to use for calculating exponential back off, see IKEv2 RETRANSMISSION</h1><h1 id="in-strongswan-conf-5"><a href="#in-strongswan-conf-5" class="headerlink" title="in strongswan.conf(5)."></a>in strongswan.conf(5).</h1><h1 id="retransmit-base-1-8"><a href="#retransmit-base-1-8" class="headerlink" title="retransmit_base = 1.8"></a>retransmit_base = 1.8</h1><h1 id="Timeout-in-seconds-before-sending-first-retransmit"><a href="#Timeout-in-seconds-before-sending-first-retransmit" class="headerlink" title="Timeout in seconds before sending first retransmit."></a>Timeout in seconds before sending first retransmit.</h1><p>retransmit_timeout = 4.0</p>
<h1 id="Number-of-times-to-retransmit-a-packet-before-giving-up"><a href="#Number-of-times-to-retransmit-a-packet-before-giving-up" class="headerlink" title="Number of times to retransmit a packet before giving up."></a>Number of times to retransmit a packet before giving up.</h1><p>retransmit_tries = 5</p>
<h1 id="Interval-to-use-when-retrying-to-initiate-an-IKE-SA-e-g-if-DNS"><a href="#Interval-to-use-when-retrying-to-initiate-an-IKE-SA-e-g-if-DNS" class="headerlink" title="Interval to use when retrying to initiate an IKE_SA (e.g. if DNS"></a>Interval to use when retrying to initiate an IKE_SA (e.g. if DNS</h1><h1 id="resolution-failed-0-to-disable-retries"><a href="#resolution-failed-0-to-disable-retries" class="headerlink" title="resolution failed), 0 to disable retries."></a>resolution failed), 0 to disable retries.</h1><h1 id="retry-initiate-interval-0"><a href="#retry-initiate-interval-0" class="headerlink" title="retry_initiate_interval = 0"></a>retry_initiate_interval = 0</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">通过man strongswan.conf 查找了超时时间一节：</span><br></pre></td></tr></table></figure>
<p>IKEv2 RETRANSMISSION<br> Retransmission timeouts in the IKEv2 daemon charon can be configured globally using the three keys listed below:</p>
<pre><code>charon.retransmit_base [1.8]
charon.retransmit_timeout [4.0]
charon.retransmit_tries [5]
charon.retransmit_jitter [0]
charon.retransmit_limit [0]
</code></pre><p> The following algorithm is used to calculate the timeout:</p>
<pre><code>relative timeout = retransmit_timeout * retransmit_base ^ (n-1)
</code></pre><p> Where  n  is  the  current  retransmission count. The calculated timeout can’t exceed the configured retransmit_limit (if<br> any), which is useful if the number of retries is high.</p>
<p> If a jitter in percent is configured, the timeout is modified as follows:</p>
<pre><code>relative timeout -= random(0, retransmit_jitter * relative timeout)
</code></pre><p> Using the default values, packets are retransmitted in:</p>
<p> Retransmission   Relative Timeout   Absolute Timeout<br> ─────────────────────────────────────────────────────<br> 1                              4s                 4s<br> 2                              7s                11s<br> 3                             13s                24s<br> 4                             23s                47s<br> 5                             42s                89s<br> giving up                     76s               165s</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">根据我们跟踪的日志信息，发下上述时间跟日志输出的时间间隔一致。考虑到安全问题，IPSec这样设计超时也许没问题，在我们的接入应用中，这么长的超时时间没法接受，我们把两个参数修改了</span><br></pre></td></tr></table></figure>
<h1 id="Timeout-in-seconds-before-sending-first-retransmit-1"><a href="#Timeout-in-seconds-before-sending-first-retransmit-1" class="headerlink" title="Timeout in seconds before sending first retransmit."></a>Timeout in seconds before sending first retransmit.</h1><p>retransmit_timeout = 3.0</p>
<h1 id="Number-of-times-to-retransmit-a-packet-before-giving-up-1"><a href="#Number-of-times-to-retransmit-a-packet-before-giving-up-1" class="headerlink" title="Number of times to retransmit a packet before giving up."></a>Number of times to retransmit a packet before giving up.</h1><p>retransmit_tries = 3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">根据它的超时算法，修改后的超时时间为：</span><br></pre></td></tr></table></figure>
<p>Retransmission   Relative Timeout   Absolute Timeout<br>─────────────────────────────────────────────────────<br>1                              3s                 3s<br>2                            5.4s               8.4s<br>3                            9.7s              18.1s<br>giving up                   17.5s              35.6s</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">    这样已经能兼顾丢包和处理效率，在局域网内效果已经可以保证。</span><br><span class="line"></span><br><span class="line">4. 如何根据DAE获取并匹配session-id</span><br><span class="line">    后续如果需要修改这块的逻辑，需要根据DAE报文中的session-id，查找ike_sa，应用新的授权信息。</span><br><span class="line"></span><br><span class="line">    * 已有的dae代码已经能根据用户名获取到用户名匹配的ike_sa</span><br><span class="line"></span><br><span class="line">    * SESSION-ID</span><br><span class="line">        * 根据radius的DAE报文，找到了session-id对应的attribute-type：\strongswan\src\libradius\radius_message.h,</span><br><span class="line">            * RAT_ACCT_SESSION_ID = 44,</span><br><span class="line">        </span><br><span class="line">        * 初始化:</span><br><span class="line">            在发送Accounting-Start时开始获取或创建：get_or_create</span><br></pre></td></tr></table></figure>
<pre><code>/**
* Send an accounting start message
*/
static void send_start(private_eap_radius_accounting_t *this, ike_sa_t *ike_sa)
{
    ...

    entry_t *entry;

    ...

    entry = get_or_create_entry(this, ike_sa-&gt;get_id(ike_sa),
                                ike_sa-&gt;get_unique_id(ike_sa));

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建时，实际调用了这些操作(unique会记录到syslog中)：</span><br></pre></td></tr></table></figure>

snprintf(entry-&gt;sid, sizeof(entry-&gt;sid), &quot;%u-%u&quot;, this-&gt;prefix, unique);
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">其中的prefix信息是这么生成的：</span><br></pre></td></tr></table></figure>

eap_radius_accounting_t *eap_radius_accounting_create()
{
    ...

    /* use system time as Session ID prefix */
    .prefix = (uint32_t)time(NULL),

    ...
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">unique_id是个递增的值，每创建一个ike_sa加1：</span><br></pre></td></tr></table></figure>

ike_sa_t * ike_sa_create(ike_sa_id_t *ike_sa_id, bool initiator,
        ike_version_t version)
{
    private_ike_sa_t *this;
    static refcount_t unique_id = 0;

    .unique_id = ref_get(&amp;unique_id),
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> </span><br></pre></td></tr></table></figure>

/**
* Increase refcount
*/
refcount_t ref_get(refcount_t *ref)
{
    refcount_t current;

    ref_lock-&gt;lock(ref_lock);
    current = ++(*ref);
    ref_lock-&gt;unlock(ref_lock);

    return current;
}
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 参考上述代码，仿照get_or_create 功能，仅使用get部分的代码：</span><br><span class="line">    * \src\libcharon\plugins\eap_radius\eap_radius_plugin.c: plugin_cb</span><br><span class="line">    此处为已有代码，在创建dae时已经给赋值了accounting对象：</span><br></pre></td></tr></table></figure>
</code></pre><p>  this-&gt;accounting = eap_radius_accounting_create();<br>  this-&gt;forward = eap_radius_forward_create();<br>  this-&gt;provider = eap_radius_provider_create();</p>
<p>  load_configs(this);</p>
<p>  if (lib-&gt;settings-&gt;get_bool(lib-&gt;settings,</p>
<pre><code>&quot;%s.plugins.eap-radius.dae.enable&quot;, FALSE, lib-&gt;ns))
</code></pre><p>  {</p>
<pre><code>this-&gt;dae = eap_radius_dae_create(this-&gt;accounting);
</code></pre><p>  }</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在src\libcharon\plugins\eap_radius\eap_radius_accounting.c 中添加get_session_id方法(示例，后续要加锁)：</span><br></pre></td></tr></table></figure>
<p>  static char <em> get_session_id(private_eap_radius_accounting_t </em>this,</p>
<pre><code>ike_sa_id_t *id)
</code></pre><p>  {</p>
<pre><code>entry_t *entry;
entry = this-&gt;sessions-&gt;get(this-&gt;sessions, id);
return entry-&gt;sid; //char sid[24];
</code></pre><p>  }</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在src\libcharon\plugins\eap_radius\eap_radius_dae.c中需要使用sid的地方调用(示例，后续要加锁、要判断是否为空)：</span><br></pre></td></tr></table></figure>
<p>  static char <em> get_session_id(private_eap_radius_dae_t </em>this,</p>
<pre><code>ike_sa_id_t *id)
</code></pre><p>  {</p>
<pre><code>return this-&gt;accounting-&gt;get_session_id(this-&gt;accounting,id);
</code></pre><p>  }</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">5. 其他发现</span><br><span class="line">    * account-stop 通过触发方式调用，在连接断开时，通过 up_down 调用</span><br><span class="line">    * 发送accounting时，会附带RAT_NAS_IP_ADDRESSs属性，这个值一般情况下是提供VPN服务的端口IP；当用户通过非园区网络连接时，也就是用户网络切换后，如果服务器到用户的路由切换到外网口，这个值会变成对应的外网口IP：</span><br><span class="line">        ``` </span><br><span class="line">    	Acct-Session-Id = &quot;1514557340-10756&quot;</span><br><span class="line">       	NAS-IP-Address = 218.195.95.16</span><br><span class="line">        Called-Station-Id = &quot;218.195.95.16[4500]&quot;</span><br><span class="line">        Calling-Station-Id = &quot;172.17.57.196[60232]&quot;</span><br></pre></td></tr></table></figure>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Acct-Session-Id = &quot;1514557340-10756&quot;</span><br><span class="line">   NAS-IP-Address = 111.21.65.2</span><br><span class="line">   Called-Station-Id = &quot;111.21.65.2[4500]&quot;</span><br><span class="line">   Calling-Station-Id = &quot;113.200.106.45[25978]&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>当网关收到用户侧发过来的数据时，会根据src和dst更新ike_sa中的host信息</p>
</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://whmz.github.io/2018/03/11/Strongswan-DAE/" data-id="cjemtrc2700001owibhv520s5" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-ipmarks" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/10/ipmarks/" class="article-date">
  <time datetime="2018-03-10T13:05:06.000Z" itemprop="datePublished">2018-03-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/10/ipmarks/">一个流量标记问题导致的限速Bug</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>继续之前限速脚本的话题，本次整理下处理的一个限速Bug。</p>
<p>近期对网络出口做了扩容，按监控的带宽数据来算，目前有20%左右的裕量，按说用户那边不再那么卡了，但是这周总是接到用户投诉说网速达不到，并发了相应的截图。为了排查具体的情况，在周三、四做了两次的问题排查，终于发现了问题点。本文简单记录下问题发现和排查过程。</p>
<h2 id="问题时间点"><a href="#问题时间点" class="headerlink" title="问题时间点"></a>问题时间点</h2><p>我们运营的学校在这周开学，大概从周二开始，有用户投诉带宽问题。考虑带宽和限速模块没有做过调整，本来是很确信不会有问题，毕竟已经正常运行了那么久的系统了！但是周三，我们自己的客服也在反馈这个问题，并拿到了实际测试的结果：在各种带宽的测试环境下，基本达不到预期的值，特别是在50M环境下，一般还达不到20M。这样基本说明，问题点确实存在，需要详细排查下。</p>
<h2 id="第一次检查"><a href="#第一次检查" class="headerlink" title="第一次检查"></a>第一次检查</h2><p>周三因为要解决另外的问题，在观察问题流量的同时，对系统做了次简单的检查。大致的流程如下：</p>
<ol>
<li><p>检查用户拨号后，下发的限速值是否正确<br> 这个可以通过syslog日志输出的限速参数检查，发现测速有问题的账号，实际限速参数并没有问题，上行20M，下行50M。在检查后再次测速，速率仍有较大差异。</p>
</li>
<li><p>检查限速参数是否正确应用到了tc的正确队列<br> 按上篇文章的介绍，用户IP和tc队列有个确定的对应关系，tc_class = (srcIP &amp; 0xff) + 1, 根据这个计算出用户队列，检查对应的上下行参数，正确。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> |     </span><br><span class="line"> +---(10:1daf) htb prio 1 rate 20Mbit ceil 20Mbit burst 1600b cburst 1600b </span><br><span class="line"> |             Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0) </span><br><span class="line"> |             rate 0bit 0pps backlog 0b 0p requeues 0 </span><br><span class="line">--</span><br><span class="line"> |             rate 0bit 0pps backlog 0b 0p requeues 0 </span><br><span class="line"> |     </span><br><span class="line"> +---(10:1daf) htb prio 1 rate 50Mbit ceil 50Mbit burst 1600b cburst 1600b </span><br><span class="line"> |             Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0) </span><br><span class="line"> |             rate 0bit 0pps backlog 0b 0p requeues 0</span><br></pre></td></tr></table></figure>
</li>
<li><p>经过这两步检查，发现用户速率没有问题，这时候怀疑是不是上行带宽实际上没有扩容？<br> 使用了<a href="https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py" target="_blank" rel="noopener">speettest的脚本</a> 检查服务器带宽：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py</span><br><span class="line">python speedtest.py</span><br></pre></td></tr></table></figure>
<p> 检查发现，在当时负载比较高、接近扩容前带宽的情况下，速率仍能达到1957Mbps，可以判断出口带宽已经扩容了。</p>
</li>
<li><p>这样，排除外部原因，开始检查下校园网络的情况。因为接入方式流量跑在校园网上，如果校园网情况不好，带宽也起不来。<br> 安排了客服下载校园内服务器的资源，可以跑满100M….</p>
</li>
</ol>
<p>经过这几步排查，发现问题确实存在，但排除了下发错误、限速参数设置、出口带宽、内部网络故障这几个原因，有可能是服务程序问题。这时候也有客服反馈，现场有单个账号出问题的，只有某个账号速率不足，50M只能到10Mbps，同样环境换账号就可以。这样差不多可以怀疑限速功能是有问题了。</p>
<p>之后是校园停电，12点了，没有办法继续试验排查，再加上有其他问题检查解决，也就暂时放下了这个问题。</p>
<h2 id="第二次检查"><a href="#第二次检查" class="headerlink" title="第二次检查"></a>第二次检查</h2><p>周四，在解决了其他问题后，重新开始检查限速的问题。这时候，因为排除了一些因素，重点放到了限速的功能上，但是还没相信是代码问题….大致做了这么些检查：</p>
<ol>
<li><p>确定一个账号先检查<br> 在这个用户登录后，获取其登录IP地址，10.151.25.99, 换算其tc-class为 10:1965, 检查其tc限速参数</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[]@vser:~$ sudo /etc/xlipsec/mark_tc.sh status ens3f0 ens3f1| grep -C 2 10:1965</span><br><span class="line"> |             rate 0bit 0pps backlog 0b 0p requeues 0 </span><br><span class="line"> |     </span><br><span class="line"> +---(10:1965) htb prio 1 rate 20Mbit ceil 20Mbit burst 1600b cburst 1600b </span><br><span class="line"> |             Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0) </span><br><span class="line"> |             rate 0bit 0pps backlog 0b 0p requeues 0 </span><br><span class="line">--</span><br><span class="line"> |             rate 0bit 0pps backlog 0b 0p requeues 0 </span><br><span class="line"> |     </span><br><span class="line"> +---(10:1965) htb prio 1 rate 50Mbit ceil 50Mbit burst 1600b cburst 1600b </span><br><span class="line"> |             Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0) </span><br><span class="line"> |             rate 0bit 0pps backlog 0b 0p requeues 0</span><br></pre></td></tr></table></figure>
<p> 可以看到，限速的队列带宽没有问题，上行20M，下行50M</p>
</li>
<li><p>测速结果，上行8.05M、下行8.91M，完全不在同一水平上么<br> 这时候，我发现” Send 0 bytes 0 pkt “，而且上下行数据都是0，这说明流量根本就没走到这个队列！</p>
</li>
<li><p>考虑到限速映射过程只有两个步骤，标记、映射，先检查标记部分：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[]@vser:~$ sudo iptables -t mangle -L -v</span><br><span class="line">Chain POSTROUTING (policy ACCEPT 56G packets, 57T bytes)</span><br><span class="line">pkts bytes target     prot opt in     out     source               destination         </span><br><span class="line">18G 2164G IPMARK     all  --  any    any     10.151.0.0/18        anywhere             -j IPMARK --addr src  --and-mask 0xfff </span><br><span class="line">16G   27T IPMARK     all  --  any    any     anywhere             10.151.0.0/18        -j IPMARK --addr dst  --and-mask 0xfff</span><br></pre></td></tr></table></figure>
<p> 看到这个结果就知道是哪里出问题了。地址池使用了18位掩码，就意味着变化的部分是后14位，如果使用0xfff掩码计算用户的mark值，就会导致部分高地址用户被映射到低位区间，跟其他用户冲突。按测试账号的数据，10.151.25.99，刚好是超出范围的高位IP用户，实际会被标记为 0965，与10.151.9.99用户共用了一个限速队列。</p>
<p> 当9.99这个用户在线的时候，两人实际上共用同一个队列，共享带宽，这就是有时候测速接近20M的原因，9.99用户使用的是20M套餐；而当9.99用户下线后，这个限速队列被重置为10Mbps，这时候测速就会出现先前的数据：上行8.05M、下行8.91M 。</p>
</li>
<li><p>问题找到后，对IPMARK代码做了修改，</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iptables -t mangle -A POSTROUTING -s 10.151.0.0/18 -j IPMARK --addr src --and-mask 0x3fff</span><br><span class="line">iptables -t mangle -D POSTROUTING 1</span><br></pre></td></tr></table></figure>
<p> 这样再测试先前的账号，已经可以看到对应的 10:1965 队列已经有流量经过：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">     |     </span><br><span class="line"> +---(10:1965) htb prio 1 rate 10Mbit ceil 10Mbit burst 1600b cburst 1600b </span><br><span class="line"> |             Sent 162249444 bytes 1574854 pkt (dropped 0, overlimits 0 requeues 0) </span><br><span class="line"> |             rate 0bit 0pps backlog 0b 0p requeues 0 </span><br><span class="line">--</span><br><span class="line"> |             rate 0bit 0pps backlog 0b 0p requeues 0 </span><br><span class="line"> |     </span><br><span class="line"> +---(10:1965) htb prio 1 rate 20Mbit ceil 20Mbit burst 1600b cburst 1600b </span><br><span class="line"> |             Sent 1362668930 bytes 1770932 pkt (dropped 0, overlimits 0 requeues 0) </span><br><span class="line"> |             rate 0bit 0pps backlog 0b 0p requeues 0</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改后的测试结果是，speettest站点测速，下行只有27.58Mbps，上行为 11.30Mbps，仍然离实际限速值差距较大。<br>为了排查问题，对这个账号的限速参数做了调整，先后尝试了80M、8M、20M</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo tc class replace dev ens3f0 parent 10:1 classid 10:1995 htb rate 80Mbit ceil 80Mbit prio 1</span><br><span class="line">sudo tc class replace dev ens3f0 parent 10:1 classid 10:1995 htb rate 8Mbit ceil 8Mbit prio 1</span><br><span class="line">sudo tc class replace dev ens3f0 parent 10:1 classid 10:1995 htb rate 20Mbit ceil 20Mbit prio 1</span><br></pre></td></tr></table></figure>
<p> 测试结果显示，限速值较小时，测速结果接近限速数据，比如8M测速为6.99M，20M被限速为 16.97M； 当速率较大的时候，速率差距就很大了，比如80M测速为 24M。再看tc队列状态，每次测速，对应的tc队列数据都会增加，说明数据确实在这个队列被限速。但是对应的dropped数据始终为0，说明数据不是在这个被丢掉的。再加上当时系统负载比较低、测速时数据包延迟在7、8ms，可以推断不是在网关限速的。</p>
</li>
<li><p>联系客服换个测速站试试，试验了360，测试50M限速情况下，实际测速 5.25 * 8 = 42Mbps，考虑封包损耗，可以认为接近限速值了。另外就是，tc的dropped字段也终于出现了非0值。</p>
</li>
</ol>
<p>后续客服联系了speedtest的限速，请求排查限速的问题，怀疑是测速站点被限速了，但在等反馈结果。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li><p>mark参数问题：最早设计时，单网关考虑4096个设备，准备采用20位地址掩码，这样计算用户的mark值只需要0xfff就够了。但是后期发现，尽量给同一账号的用户分配相同地址，这样实际占用的地址数多于在线用户数，最终使用了18位地址掩码、16384个地址，相应的mark掩码就需要调整为0x3fff。但是，部署发现问题后，只在master基线修改了代码，并没有更新部署包。新服务部署时，一般不会去检查、修改某个特殊的参数；在小规模测试时，也不会触发这个问题（登录设备大于4096才会触发问题），所以直到用户规模足够，才发现了问题。</p>
</li>
<li><p>测速反应的是客户端到测速服务器的速度，只要360能达到限速值，就说明网关没限速；但对用户来说速率实际没有达到标称值。</p>
</li>
</ol>
<h2 id="改进措施"><a href="#改进措施" class="headerlink" title="改进措施"></a>改进措施</h2><ol>
<li>修改参数后，应及时评估相关改动，对所有涉及的数据做检查，修正数据；</li>
<li>修复Bug后，应及时发布新版本修改问题，整理升级脚本；</li>
<li>服务部署的版本应该登记、管理：在发现新Bug后，应该评估影响的服务器，对所有相关服务尽快执行升级；</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://whmz.github.io/2018/03/10/ipmarks/" data-id="cjelgshdo000110wiubj2r25n" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-ss-hfsc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/25/ss-hfsc/" class="article-date">
  <time datetime="2018-02-25T10:33:06.000Z" itemprop="datePublished">2018-02-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/25/ss-hfsc/">多用户环境下的流量标记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>单位使用了个Linux网关在园区网络提供旁路接入功能。作为运营网络，肯定得考虑限速的问题，使用了iptables、IPMARK、tc-htb等功能实现了限速。</p>
<p>随着用户数逐渐增多，当出口网络比较拥堵的时候，大量用户反馈延迟较高、游戏用户基本无法使用。基于这样的情况，我们尝试是否能在当前的系统上，增加QoS支持：区分用户的应用流量、执行不同速率策略，缓解问题。</p>
<p>本文简单记录了两个试验方案，使用HTB和HFSC限速和整流，给出限速的关键脚本和思路，以及配套的MARK规则。本文先不讨论HTB、HFSC本身的使用方法和设计，主要讨论分类器的设计和实现。</p>
<h2 id="基于用户的限速"><a href="#基于用户的限速" class="headerlink" title="基于用户的限速"></a>基于用户的限速</h2><p>策略比较简单，每个接入用户使用自己的htb-class：</p>
<ul>
<li>在用户接入的时候，Radius模块能收到用户的接入IP、上下行带宽数据，根据这些数据设置htb-class的限速参数</li>
<li>当用户有网络流量经过网关时，根据IP把对应的流量映射到htb-class</li>
</ul>
<p>在具体代码上，大致有这么三段：</p>
<ol>
<li><p>通过iptables-mark标记流量</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 根据流方向，用户上下行有两条规则.对于用户输入流量，假定用户IP为 srcIP ，则 </span><br><span class="line"># mark = srcIP &amp; 0xff</span><br><span class="line"></span><br><span class="line">iptables -t mangle -A POSTROUTING -s $VIPS -j IPMARK --addr src --and-mask 0xfff</span><br></pre></td></tr></table></figure>
</li>
<li><p>tc初始化    </p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 定义用户流量映射规则</span><br><span class="line"># 以mark为key，映射用户流量到 10:(1+mark) 分类</span><br><span class="line"></span><br><span class="line">tc filter add dev $ODEV parent 10: handle 100 flow map key mark baseclass 10:1</span><br></pre></td></tr></table></figure>
</li>
<li><p>tc限速设置</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 用户IP为vip，计算其vflag，对10:$vflag分类设置速率值，其中</span><br><span class="line"># vflag = ($vip + 1) &amp; 0xfff</span><br><span class="line"></span><br><span class="line">tc class replace dev $ODEV parent 10:1 classid 10:$vflag htb rate $urate ceil $uceil prio 1</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="基于用户-QoS的限速"><a href="#基于用户-QoS的限速" class="headerlink" title="基于用户+QoS的限速"></a>基于用户+QoS的限速</h2><p>根据我们的分析，除了扩大出口流量，试试对用户流量分级、降低大流量应用的优先级、保障游戏、交互带宽，应该也能缓解部分问题。</p>
<p>相对简单的用户限速方案，这个方案要在识别用户流量的基础上，进一步识别应用。考虑我们的流量规模大致在5~10Gbps，在这样的网络环境下要实现应用流量识别、再加以QoS限速，从性能上讲不大现实——算法太复杂，算法本身对性能的影响太大，会进一步加大延迟。</p>
<p>内部讨论后，我们基于包长度做了个简单的模型。除了数据包本身的QoS字段之外，包长度在一定程度上也能反映用户数据流量的特征：大数据传输一般使用最大报文长度；TCP初始连接、交互数据一般不会有太大的数据量，从而数据包长度较短。</p>
<p>考虑这个情况，我们的需求1：<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">设计一个相对简洁的标记机制，区分同一用户下的流量，使得针对不同的流量能施加不同的速率控制</span><br></pre></td></tr></table></figure></p>
<p>另外，用户接入过程也会占用一部分流量，这部分流量因为未纳入限速框架（用户接入成功前IP不在$VIP定义的地址池中），默认会归为系统流量。如果不给这部分流量保留带宽，用户在高峰期的连接成功率会比较差。需求2来了：<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">新的流量标记机制，必须区分系统带宽和用户带宽，对系统带宽也进行分类，对接入流量和其他流量设置不同策略。</span><br></pre></td></tr></table></figure></p>
<p>基于这两个需求，我们设计了这样的一个MARK规则（请忽略长度数据….）：<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># length &lt; 400</span><br><span class="line">1.  iptables -t mangle -A POSTROUTING -d $VIPS -m length --length :400 -j IPMARK --addr dst --and-mask 0x3fff --or-mask 0x24000</span><br><span class="line">2.  iptables -t mangle -A POSTROUTING -d $VIPS -m length --length :400 -j RETURN</span><br><span class="line"></span><br><span class="line"># length &gt; 400</span><br><span class="line">3.  iptables -t mangle -A POSTROUTING -d $VIPS -j IPMARK --addr dst --and-mask 0x3fff --or-mask 0x28000</span><br><span class="line">4.  iptables -t mangle -A POSTROUTING -d $VIPS -j RETURN</span><br><span class="line"></span><br><span class="line"># sys load</span><br><span class="line">5.  iptables -t mangle -A POSTROUTING -m length --length :400 -j IPMARK --set-mark 0x14000</span><br><span class="line">6.  iptables -t mangle -A POSTROUTING -m length --length :400 -j RETURN</span><br><span class="line"></span><br><span class="line">7.  iptables -t mangle -A POSTROUTING -j MARK --set-mark 0x18000</span><br><span class="line">8.  iptables -t mangle -A POSTROUTING -j RETURN</span><br></pre></td></tr></table></figure></p>
<p>按设计想法，用户流量符合$VIPS地址池条件，会在前面的四条规则中被设置为 ((ip &amp; 0x3fff) | 0x24000) 或者 ((ip &amp; 0x3fff) | 0x24000)，系统流量则归为 0x14000 和 0x18000 ，从而实现上述两个需求。同时，符合一条规则的包不再执行后续的检查，从性能上讲也好一点。</p>
<p>但是，实际测试时候发现，所有包都被纳入了 0x14000 和 0x18000 这两个分类！<br>为了跟踪包标记情况，使用了iptables -t mangle -L -v，查看了每条规则的包匹配情况，发现每个经过1、2的包，都会再次经过5、6; 而3、4两条规则也是一样，再次经过7、8。这样的话，每个包被标记了两次，肯定是后一个标记起作用。</p>
<p>那么问题是，为什么包在规则2、4行的RETURN之后，还会继续经过后续的5、6、7、8规则呢？原来，我们的接入方式是IPSec，一个网络包进入网卡后，会根据数据方向，通过XFRM框架解包/封包，在协议栈内部实际跑两圈，两次经过mangle postrouting链。在用户下行数据方向，就会出现刚才的情况，实际上所有流量都会走到系统队列处理。这个可以参考<a href="https://upload.wikimedia.org/wikipedia/commons/3/37/Netfilter-packet-flow.svg" target="_blank" rel="noopener">Netfilter-packet-flow</a></p>
<p>为了解决这个问题，修改后的的MARK规则是这样的:<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># ip addr</span><br><span class="line">iptables -t mangle -A POSTROUTING -d $VIPS -j IPMARK --addr dst --and-mask 0x3fff --or-mask 0x20000</span><br><span class="line"></span><br><span class="line"># load length</span><br><span class="line">iptables -t mangle -A POSTROUTING -m length --length :400 -j MARK --or-mark 0x14000</span><br><span class="line">iptables -t mangle -A POSTROUTING -m length --length :400 -j RETURN</span><br><span class="line"></span><br><span class="line">iptables -t mangle -A POSTROUTING -j MARK --or-mark 0x18000</span><br><span class="line">iptables -t mangle -A POSTROUTING -j RETURN</span><br></pre></td></tr></table></figure></p>
<p>按这个规则，地址池$VIPS中的地址包，会在原有的mark基础上添加一个0x20000; 所有的包都会根据包长度，在原有mark上添加一个 0x14000 或 0x18000 。这样，用户包，始终都会经过两次mark标记，最终结果为 0x34000 或 0x38000 ；系统包因为不会满足$VIPS的地址池条件，其mark只能是 0x14000 或 0x18000 。该标记满足了上述两个需求。</p>
<p>但是这样一来，每个用户包已经至少多执行3条过滤规则</p>
<h2 id="需要进一步学习的内容"><a href="#需要进一步学习的内容" class="headerlink" title="需要进一步学习的内容"></a>需要进一步学习的内容</h2><p>hfsc限速参数中，rt、ls、ul、sc的含义和相互关系。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://whmz.github.io/2018/02/25/ss-hfsc/" data-id="cjelgshdv000210wif714xv8o" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hello" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/25/hello/" class="article-date">
  <time datetime="2018-02-25T10:28:45.000Z" itemprop="datePublished">2018-02-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/25/hello/">开始！</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>用作个人知识整理和分享的Blog，目前规划这些内容：</p>
<ul>
<li>知识备忘：近段时间了解的新知识，稍作探索的摘要，或简单记录用作备忘。</li>
<li>学习整理：近期内学习、掌握的知识和技能点，做部分整理，以用作经验分享。</li>
<li>问题记录：记录平时工作学习解决的问题。一般来源于工作实践，记录处理具体问题的过程和方法。</li>
<li>学习计划：近段时间预计学习的知识点</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://whmz.github.io/2018/02/25/hello/" data-id="cjelgshc4000010wihwe3tml4" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">二月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/03/11/Strongswan-DAE/">Strongswan-DAE功能相关逻辑整理</a>
          </li>
        
          <li>
            <a href="/2018/03/10/ipmarks/">一个流量标记问题导致的限速Bug</a>
          </li>
        
          <li>
            <a href="/2018/02/25/ss-hfsc/">多用户环境下的流量标记</a>
          </li>
        
          <li>
            <a href="/2018/02/25/hello/">开始！</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 whmz?<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>